<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Patrick Feeney</title><link href="https://patrickfeeney.github.io/" rel="alternate"></link><link href="https://patrickfeeney.github.io/feeds/all.atom.xml" rel="self"></link><id>https://patrickfeeney.github.io/</id><updated>2024-06-28T10:00:00-05:00</updated><entry><title>Learning Embedding Spaces with Metrics via Contrastive Learning</title><link href="https://patrickfeeney.github.io/learning-embedding-spaces-with-metrics-via-contrastive-learning.html" rel="alternate"></link><published>2024-06-28T10:00:00-05:00</published><updated>2024-06-28T10:00:00-05:00</updated><author><name>Patrick Feeney</name></author><id>tag:patrickfeeney.github.io,2024-06-28:/learning-embedding-spaces-with-metrics-via-contrastive-learning.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Published in the blogpost track of the &lt;a href="https://gram-workshop.github.io/index.html"&gt;ELLIS Workshop on Geometry-grounded Representation Learning and Generative Modeling at ICML 2024&lt;/a&gt;.
Written by Patrick Feeney and Michael C. Hughes.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Contrastive learning encompasses a variety of methods that learn a constrained embedding space to solve a task. The embedding space is constrained such …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Published in the blogpost track of the &lt;a href="https://gram-workshop.github.io/index.html"&gt;ELLIS Workshop on Geometry-grounded Representation Learning and Generative Modeling at ICML 2024&lt;/a&gt;.
Written by Patrick Feeney and Michael C. Hughes.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Contrastive learning encompasses a variety of methods that learn a constrained embedding space to solve a task. The embedding space is constrained such that a chosen metric, a function that measures the distance between two embeddings, satisfies some desired property, usually that small distances imply a shared class. Contrastive learning underlies many self-supervised methods, such as MoCo &lt;a href='#he_momentum_2020' id='ref-he_momentum_2020-1'&gt;(He et al., 2020)&lt;/a&gt;, &lt;a href='#chen_empirical_2021' id='ref-chen_empirical_2021-1'&gt;(Chen et al., 2021)&lt;/a&gt;, SimCLR &lt;a href='#chen_simple_2020' id='ref-chen_simple_2020-1'&gt;(Chen et al., 2020)&lt;/a&gt;, &lt;a href='#chen_big_2020' id='ref-chen_big_2020-1'&gt;(Chen et al., 2020)&lt;/a&gt;, and BYOL &lt;a href='#grill_bootstrap_2020' id='ref-grill_bootstrap_2020-1'&gt;(Grill et al., 2020)&lt;/a&gt;, as well as supervised methods such as SupCon &lt;a href='#khosla_supervised_2020' id='ref-khosla_supervised_2020-1'&gt;(Khosla et al., 2020)&lt;/a&gt; and SINCERE &lt;a href='#feeney_sincere_2024' id='ref-feeney_sincere_2024-1'&gt;(Feeney and Hughes, 2024)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In contrastive learning, there are two components that determine the constraints on the learned embedding space: the similarity function and the contrastive loss. The similarity function takes a pair of embedding vectors and quantifies how similar they are as a scalar. The contrastive loss determines which pairs of embeddings have similarity evaluated and how the resulting set of similarity values are used to measure error with respect to a task, such as classification. Backpropagating to minimize this error causes a model to learn embeddings that best satisfy the constraints induced by the similarity function and contrastive loss.&lt;/p&gt;
&lt;p&gt;This blog post examines how similarity functions and contrastive losses affect the learned embedding spaces. We first examine the different choices for similarity functions and contrastive losses. Then we conclude with a brief case study investigating the effects of different similarity functions on supervised contrastive learning.&lt;/p&gt;
&lt;h2&gt;Similarity Functions&lt;/h2&gt;
&lt;p&gt;A similarity function &lt;span class="math"&gt;\(s(z_1, z_2): \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}\)&lt;/span&gt; maps a pair of &lt;span class="math"&gt;\(d\)&lt;/span&gt;-dimensional embedding vectors &lt;span class="math"&gt;\(z_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(z_2\)&lt;/span&gt; to a real similarity value, with greater values indicating greater similarity. A temperature hyperparameter &lt;span class="math"&gt;\(0 &amp;lt; \tau \leq 1\)&lt;/span&gt; is often included, via &lt;span class="math"&gt;\(\frac{s(z_1, z_2)}{\tau}\)&lt;/span&gt;, to scale a similarity function. If the similarity function has a range that is a subset of &lt;span class="math"&gt;\(\mathbb{R}\)&lt;/span&gt;, then &lt;span class="math"&gt;\(\tau\)&lt;/span&gt; can increase that range. &lt;span class="math"&gt;\(\tau\)&lt;/span&gt; is omitted for simplicity here.&lt;/p&gt;
&lt;h3&gt;Cosine Similarity&lt;/h3&gt;
&lt;p&gt;A common similarity function is cosine similarity&lt;/p&gt;
&lt;div class="math"&gt;$$s(z_1, z_2) = \frac{z_1 \cdot z_2}{||z_1|| \cdot ||z_2||}.$$&lt;/div&gt;
&lt;p&gt;This function measures the cosine of the angle between &lt;span class="math"&gt;\(z_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(z_2\)&lt;/span&gt; as a scalar in &lt;span class="math"&gt;\([-1, 1]\)&lt;/span&gt;. Cosine similarity violates the triangle inequality, making it the only similarity function discussed here that is not derived from a distance metric.&lt;/p&gt;
&lt;h3&gt;Negative Arc Length&lt;/h3&gt;
&lt;p&gt;The recently proposed negative arc length similarity function &lt;a href='#koishekenov_geometric_2023' id='ref-koishekenov_geometric_2023-1'&gt;(Koishekenov et al., 2023)&lt;/a&gt; provides an analogue for cosine similarity that is a distance metric&lt;/p&gt;
&lt;div class="math"&gt;$$s(z_1, z_2) = 1 - \frac{\text{arccos}(z_1 \cdot z_2)}{\pi}.$$&lt;/div&gt;
&lt;p&gt;This function assumes that &lt;span class="math"&gt;\(||z_1|| = ||z_2|| = 1\)&lt;/span&gt; which is a common normalization &lt;a href='#le-khac_contrastive_2020' id='ref-le-khac_contrastive_2020-1'&gt;(Le-Khac et al., 2020)&lt;/a&gt; that restricts the embeddings to a hypersphere. The arc length &lt;span class="math"&gt;\(\text{arccos}(z_1 \cdot z_2)\)&lt;/span&gt; is a natural choice for comparing such vectors as it is the geodesic distance, or the length of the shortest path between &lt;span class="math"&gt;\(z_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(z_2\)&lt;/span&gt; on the hypersphere. Subtracting the arc length converts the distance metric into a similarity function with range &lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Negative Euclidean Distance&lt;/h3&gt;
&lt;p&gt;The negative Euclidean distance similarity function is simply&lt;/p&gt;
&lt;div class="math"&gt;$$s(z_1, z_2) = -||z_1 - z_2||_2.$$&lt;/div&gt;
&lt;p&gt;Euclidean distance measures the shortest path in Euclidean space, making it the geodesic distance when &lt;span class="math"&gt;\(z_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(z_2\)&lt;/span&gt; can take any value in &lt;span class="math"&gt;\(\mathbb{R}^d\)&lt;/span&gt;. In this case the similarity function has range &lt;span class="math"&gt;\([-\infty, 0]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The negative Euclidean distance can also be used with embeddings restricted to a hypersphere, resulting in range &lt;span class="math"&gt;\([-2, 0]\)&lt;/span&gt;. However, this is not the geodesic distance for the hypersphere as the path being measured is inside the sphere. The Euclidean distance will be less than the arc length unless &lt;span class="math"&gt;\(z_1 = z_2\)&lt;/span&gt;, in which case they both equal 0.&lt;/p&gt;
&lt;h2&gt;Contrastive Losses&lt;/h2&gt;
&lt;p&gt;A contrastive loss function maps a set of embeddings and a similarity function to a scalar value. Losses are written such that derivatives for backpropagation are taken with respect to the embedding &lt;span class="math"&gt;\(z\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Margin Losses&lt;/h3&gt;
&lt;p&gt;The original contrastive loss &lt;a href='#chopra_learning_2005' id='ref-chopra_learning_2005-1'&gt;(Chopra et al., 2005)&lt;/a&gt; maximizes similarity for examples &lt;span class="math"&gt;\(z^+\)&lt;/span&gt; and minimizes similarity for examples &lt;span class="math"&gt;\(z^-\)&lt;/span&gt;, until the similarity is below margin hyperparameter &lt;span class="math"&gt;\(m\)&lt;/span&gt;, via&lt;/p&gt;
&lt;div class="math"&gt;$$L(z, z^+) = s(z, z^+); L(z, z^-) = \max( 0, m - s(z, z^-) ).$$&lt;/div&gt;
&lt;p&gt;The structure of this loss implies that &lt;span class="math"&gt;\(z_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(z_2\)&lt;/span&gt; share a class if &lt;span class="math"&gt;\(s(z_1, z_2) &amp;lt; m\)&lt;/span&gt; and otherwise they do not share a class. This margin hyperparameter can be challenging to tune for efficiency throughout the training process because it needs to be satisfiable but also provide &lt;span class="math"&gt;\(z^-\)&lt;/span&gt; samples within the margin in order to backpropagate the error.&lt;/p&gt;
&lt;p&gt;The triplet loss &lt;a href='#schroff_facenet_2015' id='ref-schroff_facenet_2015-1'&gt;(Schroff et al., 2015)&lt;/a&gt; avoids this by using a margin between similarity values&lt;/p&gt;
&lt;div class="math"&gt;$$L(z, z^+, z^-) = \max( 0, s(z, z^+) - s(z, z^-) + m).$$&lt;/div&gt;
&lt;p&gt;The triplet loss only updates a network when its loss is positive, so finding triplets satisfying that condition are important for learning efficiency.&lt;/p&gt;
&lt;p&gt;Lifted Structured Loss &lt;a href='#oh_song_deep_2016' id='ref-oh_song_deep_2016-1'&gt;(OhSong et al., 2016)&lt;/a&gt; handles this by precomputing similarities for all pairs in a batch then selecting the &lt;span class="math"&gt;\(z^-\)&lt;/span&gt; with maximal similarity&lt;/p&gt;
&lt;div class="math"&gt;$$L(z, z^+) = \max( 0, s(z, z^+) + m - \max [ \max_{z^-} s(z, z^-), \max_{z^-} s(z^+, z^-) ] ).$$&lt;/div&gt;
&lt;p&gt;The Batch Hard loss &lt;a href='#hermans_defense_2017' id='ref-hermans_defense_2017-1'&gt;(Hermans et al., 2017)&lt;/a&gt; takes this even further by selecting &lt;span class="math"&gt;\(z^+\)&lt;/span&gt; with minimal similarity&lt;/p&gt;
&lt;div class="math"&gt;$$L(z, z^+) = \max( 0, \min_{z^+} [ s(z, z^+) ] + m - \max_{z^-} [ s(z, z^-) ] ).$$&lt;/div&gt;
&lt;p&gt;The decision to compute the loss based on comparisons between &lt;span class="math"&gt;\(z\)&lt;/span&gt;, a single &lt;span class="math"&gt;\(z^+\)&lt;/span&gt;, and a single &lt;span class="math"&gt;\(z^-\)&lt;/span&gt; comes with advantages and disadvantages. These methods can be easier to adapt for learning with varying levels of supervision because complete knowledge of whether similarity should be maximized or minimized for each pair in the dataset is not required. However, these methods also make training efficiently difficult and provide relatively loose constraints on the embedding space.&lt;/p&gt;
&lt;h3&gt;Cross Entropy Losses&lt;/h3&gt;
&lt;p&gt;A common contrastive loss is the Information Noise Contrastive Estimation (InfoNCE) &lt;a href='#oord_representation_2019' id='ref-oord_representation_2019-1'&gt;(Oord et al., 2019)&lt;/a&gt; loss&lt;/p&gt;
&lt;div class="math"&gt;$$L(z, z^+, z^-_1, z^-_2, \ldots, z^-_n) = -\log \frac{ e^{s(z, z^+)} }{ e^{s(z, z^+)} + \sum_{i=1}^n e^{s(z, z^-_i)} }.$$&lt;/div&gt;
&lt;p&gt;InfoNCE is a cross entropy loss whose logits are similarities for &lt;span class="math"&gt;\(z\)&lt;/span&gt;. &lt;span class="math"&gt;\(z^+\)&lt;/span&gt; is a single embedding whose similarity with &lt;span class="math"&gt;\(z\)&lt;/span&gt; should be maximized while &lt;span class="math"&gt;\(z^-_1, z^-_2, \ldots, z^-_n\)&lt;/span&gt; are a set of &lt;span class="math"&gt;\(n\)&lt;/span&gt; embeddings whose similarity with &lt;span class="math"&gt;\(z\)&lt;/span&gt; should be minimized. The structure of this loss implies that &lt;span class="math"&gt;\(z_1\)&lt;/span&gt; shares a class with &lt;span class="math"&gt;\(z_2\)&lt;/span&gt; if no other embedding has greater similarity with &lt;span class="math"&gt;\(z_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The choice of &lt;span class="math"&gt;\(z^+\)&lt;/span&gt; and &lt;span class="math"&gt;\(z^-\)&lt;/span&gt; sets varies across methods. The self-supervised InfoNCE loss chooses &lt;span class="math"&gt;\(z^+\)&lt;/span&gt; to be an embedding of an augmentation of the input that produced &lt;span class="math"&gt;\(z\)&lt;/span&gt; and &lt;span class="math"&gt;\(z^-\)&lt;/span&gt; to be the other inputs and augmentations in the batch. This is called instance discrimination because only augmentations of the same input instance have their similarity maximized.&lt;/p&gt;
&lt;p&gt;Supervised methods expand the definition of &lt;span class="math"&gt;\(z^+\)&lt;/span&gt; to also include embeddings which share a class with &lt;span class="math"&gt;\(z\)&lt;/span&gt;. The expectation of InfoNCE loss over choices of &lt;span class="math"&gt;\(z^+\)&lt;/span&gt; is used to jointly maximize their similarity to &lt;span class="math"&gt;\(z\)&lt;/span&gt;. The Supervised Contrastive (SupCon) loss &lt;a href='#khosla_supervised_2020' id='ref-khosla_supervised_2020-2'&gt;(Khosla et al., 2020)&lt;/a&gt; uses all embeddings not currently set as &lt;span class="math"&gt;\(z\)&lt;/span&gt; or &lt;span class="math"&gt;\(z^+\)&lt;/span&gt; as &lt;span class="math"&gt;\(z^-\)&lt;/span&gt;, including embeddings that share a class with &lt;span class="math"&gt;\(z\)&lt;/span&gt; and therefore will also be used as &lt;span class="math"&gt;\(z^+\)&lt;/span&gt;. This creates loss terms that would minimize similarity between embeddings that share a class. Supervised Information Noise-Contrastive Estimation REvisited (SINCERE) loss &lt;a href='#feeney_sincere_2024' id='ref-feeney_sincere_2024-2'&gt;(Feeney and Hughes, 2024)&lt;/a&gt; removes embeddings that share a class with &lt;span class="math"&gt;\(z\)&lt;/span&gt; from &lt;span class="math"&gt;\(z^-\)&lt;/span&gt;, leaving only embeddings with different classes. An additional margin hyperparameter can also be added to these losses &lt;a href='#barbano_unbiased_2022' id='ref-barbano_unbiased_2022-1'&gt;(Barbano et al., 2022)&lt;/a&gt;, which allows for interpolation between the original losses and losses with the &lt;span class="math"&gt;\(e^{s(z, z^+)}\)&lt;/span&gt; term removed from the denominator.&lt;/p&gt;
&lt;p&gt;Considering a set of similarities during loss calculation allows the loss to implicitly perform hard negative mining &lt;a href='#khosla_supervised_2020' id='ref-khosla_supervised_2020-3'&gt;(Khosla et al., 2020)&lt;/a&gt;, avoiding the challenge of selecting triplets required by a margin loss. The lack of a margin places strict constraints on the embedding space, as similarities are always being pushed towards the maximum or minimum. This enables analysis of embedding spaces that minimize the loss. For example, InfoNCE and SINCERE losses with cosine similarity are minimized by embedding spaces with clusters of inputs mapped to single points (maximizing similarity) that are uniformly distributed on the unit sphere (minimizing similarity) &lt;a href='#wang_understanding_nodate' id='ref-wang_understanding_nodate-1'&gt;(Wang and Isola, 2020)&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Case Study: Contrastive Learning on a Hypersphere&lt;/h2&gt;
&lt;p&gt;Many modern contrastive learning techniques build off of the combination of cosine similarity and cross entropy losses. However, few papers have explored changing similarity functions and losses outside of the context of a more complex model.&lt;/p&gt;
&lt;p&gt;&lt;a href='#koishekenov_geometric_2023' id='ref-koishekenov_geometric_2023-2'&gt;Koishekenov et al. (2023)&lt;/a&gt; recently reported improved downstream performance by replacing cosine similarity with negative arc length for two self-supervised cross entropy losses. This change is motivated by the desire to use the geodesic distance on the embedding space, which in this case is a unit hypersphere. We investigate whether replacing cosine similarity with negative arc length similarity can improve performance with the SINCERE loss, which is supervised, and how each similarity affects the learned embedding space.&lt;/p&gt;
&lt;h3&gt;Supervised Learning Accuracy&lt;/h3&gt;
&lt;p&gt;We utilize the methodology of &lt;a href='#feeney_sincere_2024' id='ref-feeney_sincere_2024-3'&gt;Feeney and Hughes (2024)&lt;/a&gt; to evaluate if the results of &lt;a href='#koishekenov_geometric_2023' id='ref-koishekenov_geometric_2023-3'&gt;Koishekenov et al. (2023)&lt;/a&gt; generalize to supervised cross entropy losses. Specifically, we train models with SINCERE loss and each similarity function then evaluate the models with nearest neighbor classifiers on the test set.&lt;/p&gt;
&lt;style&gt;
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
  padding: 5px;
}
&lt;/style&gt;
&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td colspan="2" &gt;&lt;strong&gt;CIFAR-10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan="2" &gt;&lt;strong&gt;CIFAR-100&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Similarity&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1NN&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;5NN&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1NN&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;5NN&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Cosine
   &lt;/td&gt;
   &lt;td&gt;95.88
   &lt;/td&gt;
   &lt;td&gt;95.91
   &lt;/td&gt;
   &lt;td&gt;76.23
   &lt;/td&gt;
   &lt;td&gt;76.13
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Negative Arc Length
   &lt;/td&gt;
   &lt;td&gt;95.66
   &lt;/td&gt;
   &lt;td&gt;95.65
   &lt;/td&gt;
   &lt;td&gt;75.81
   &lt;/td&gt;
   &lt;td&gt;76.41
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We find no statistically significant difference based on the 95% confidence interval of the accuracy difference &lt;a href='#foody_classification_2009' id='ref-foody_classification_2009-1'&gt;(Foody, 2009)&lt;/a&gt; from 1,000 iterations of test set bootstrapping. This aligns with the results in &lt;a href='#feeney_sincere_2024' id='ref-feeney_sincere_2024-4'&gt;Feeney and Hughes (2024)&lt;/a&gt;, which found a similar lack of statistically significant results across choices of supervised contrastive cross entropy losses. This suggests that supervised learning accuracy is similar across choices of reasonable similarity functions and contrastive losses.&lt;/p&gt;
&lt;h3&gt;Supervised Learning Embedding Space&lt;/h3&gt;
&lt;p&gt;We also visualize the learned embedding space for each CIFAR-10 model. For each test set image, the similarity value is plotted for the closest training set image that shares a class (“Target”) and that does not share a class (“Noise”). This visualizes the 1-nearest neighbor decision process. Both similarity functions are plotted for each model, with the title denoting the similarity function used during training.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://patrickfeeney.github.io/blog/learning_embedding_spaces_assets/cos_cifar10_all_cos.png"&gt;
&lt;img src="https://patrickfeeney.github.io/blog/learning_embedding_spaces_assets/cos_cifar10_all_arccos.png"&gt;&lt;/p&gt;
&lt;p&gt;The model trained with cosine similarity maximizes the similarity to target images well. There are a small number of noise images with near maximal similarity, but the majority are below 0.3 cosine similarity. Interestingly, the peaks seen in the noise similarity reflects the fact that individual classes will have different modes of their noise histograms.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://patrickfeeney.github.io/blog/learning_embedding_spaces_assets/arccos_cifar10_all_cos.png"&gt;
&lt;img src="https://patrickfeeney.github.io/blog/learning_embedding_spaces_assets/arccos_cifar10_all_arccos.png"&gt;&lt;/p&gt;
&lt;p&gt;The model trained with negative arc length similarity does a better job of forcing target similarity values very close to 1 negative arc length similarity, but also has a notable number of target similarities near 0.5 negative arc length similarity. The noise distribution also reflects the fact that individual classes have different modes for their noise histograms, but in this case the modes are spread across more similarity values. Notably the peak for the horse class is very close to the max similarity due to a high similarity to the dog class, although they are still separated enough from the target similarities to not have an impact on accuracy.&lt;/p&gt;
&lt;h3&gt;Discussion&lt;/h3&gt;
&lt;p&gt;The choice of similarity function clearly has an effect on the learned embedding space despite a lack of statistically significant changes in accuracy. The cosine similarity histogram most cleanly aligns with the intuition that contrastive losses should be maximizing and minimizing similarities. In contrast, the negative arc length similarity histogram suggests similarity minimization is sacrificed for very consistent maximization. These differences in the learned embedding spaces could affect performance on downstream tasks such as transfer learning.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%&amp;#64;#$&amp;#64;#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;hr&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p id='barbano_unbiased_2022'&gt;Carlo&amp;nbsp;Alberto Barbano, Benoit Dufumier, Enzo Tartaglione, Marco Grangetto, and Pietro Gori.
Unbiased supervised contrastive learning.
In &lt;em&gt;The Eleventh International Conference on Learning Representations&lt;/em&gt;. September 2022.
URL: &lt;a href="https://openreview.net/forum?id=Ph5cJSfD2XN"&gt;https://openreview.net/forum?id=Ph5cJSfD2XN&lt;/a&gt; (visited on 2024-05-28). &lt;a class="cite-backref" href="#ref-barbano_unbiased_2022-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='chen_simple_2020'&gt;Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
A simple framework for contrastive learning of visual representations.
In &lt;em&gt;Proceedings of the 37th International Conference on Machine Learning&lt;/em&gt;, 1597–1607. PMLR, November 2020.
ISSN: 2640-3498.
URL: &lt;a href="https://proceedings.mlr.press/v119/chen20j.html"&gt;https://proceedings.mlr.press/v119/chen20j.html&lt;/a&gt; (visited on 2023-02-07). &lt;a class="cite-backref" href="#ref-chen_simple_2020-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='chen_big_2020'&gt;Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey&amp;nbsp;E Hinton.
Big self-supervised models are strong semi-supervised learners.
In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, volume&amp;nbsp;33, 22243–22255. Curran Associates, Inc., 2020.
URL: &lt;a href="https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html"&gt;https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html&lt;/a&gt; (visited on 2023-07-28). &lt;a class="cite-backref" href="#ref-chen_big_2020-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='chen_empirical_2021'&gt;Xinlei Chen, Saining Xie, and Kaiming He.
An empirical study of training self-supervised vision transformers.
In &lt;em&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/em&gt;, 9640–9649. 2021.
URL: &lt;a href="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html"&gt;https://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html&lt;/a&gt; (visited on 2023-02-09). &lt;a class="cite-backref" href="#ref-chen_empirical_2021-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='chopra_learning_2005'&gt;S.&amp;nbsp;Chopra, R.&amp;nbsp;Hadsell, and Y.&amp;nbsp;LeCun.
Learning a similarity metric discriminatively, with application to face verification.
In &lt;em&gt;2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)&lt;/em&gt;, volume&amp;nbsp;1, 539–546 vol. 1. June 2005.
ISSN: 1063-6919.
URL: &lt;a href="https://ieeexplore.ieee.org/abstract/document/1467314?casa_token=fbldvqZUi0gAAAAA:sIzRPsJfUiIGmux-j7j6aoov1PNxo944AwjVUdSzTh47MueCdAM3fLFyFf20oLu2lr33z_ph"&gt;https://ieeexplore.ieee.org/abstract/document/1467314?casa_token=fbldvqZUi0gAAAAA:sIzRPsJfUiIGmux-j7j6aoov1PNxo944AwjVUdSzTh47MueCdAM3fLFyFf20oLu2lr33z_ph&lt;/a&gt; (visited on 2024-05-28), &lt;a href="https://doi.org/10.1109/CVPR.2005.202"&gt;doi:10.1109/CVPR.2005.202&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-chopra_learning_2005-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='feeney_sincere_2024'&gt;Patrick Feeney and Michael&amp;nbsp;C. Hughes.
Sincere: supervised information noise-contrastive estimation revisited.
February 2024.
arXiv:2309.14277 [cs].
URL: &lt;a href="http://arxiv.org/abs/2309.14277"&gt;http://arxiv.org/abs/2309.14277&lt;/a&gt; (visited on 2024-03-20), &lt;a href="https://doi.org/10.48550/arXiv.2309.14277"&gt;doi:10.48550/arXiv.2309.14277&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-feeney_sincere_2024-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-feeney_sincere_2024-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-feeney_sincere_2024-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-feeney_sincere_2024-3" title="Jump back to reference 3"&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-feeney_sincere_2024-4" title="Jump back to reference 4"&gt;&lt;sup&gt;4&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='foody_classification_2009'&gt;Giles&amp;nbsp;M. Foody.
Classification accuracy comparison: hypothesis tests and the use of confidence intervals in evaluations of difference, equivalence and non-inferiority.
&lt;em&gt;Remote Sensing of Environment&lt;/em&gt;, 113(8):1658–1663, August 2009.
URL: &lt;a href="https://www.sciencedirect.com/science/article/pii/S0034425709000923"&gt;https://www.sciencedirect.com/science/article/pii/S0034425709000923&lt;/a&gt; (visited on 2024-05-28), &lt;a href="https://doi.org/10.1016/j.rse.2009.03.014"&gt;doi:10.1016/j.rse.2009.03.014&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-foody_classification_2009-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='grill_bootstrap_2020'&gt;Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila&amp;nbsp;Pires, Zhaohan Guo, Mohammad Gheshlaghi&amp;nbsp;Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko.
Bootstrap your own latent - a new approach to self-supervised learning.
In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, volume&amp;nbsp;33, 21271–21284. Curran Associates, Inc., 2020.
URL: &lt;a href="https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html"&gt;https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html&lt;/a&gt; (visited on 2023-03-08). &lt;a class="cite-backref" href="#ref-grill_bootstrap_2020-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='he_momentum_2020'&gt;Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
Momentum contrast for unsupervised visual representation learning.
In &lt;em&gt;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&lt;/em&gt;, 9729–9738. 2020.
URL: &lt;a href="https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html"&gt;https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html&lt;/a&gt; (visited on 2023-02-07). &lt;a class="cite-backref" href="#ref-he_momentum_2020-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='hermans_defense_2017'&gt;Alexander Hermans, Lucas Beyer, and Bastian Leibe.
In defense of the triplet loss for person re-identification.
November 2017.
arXiv:1703.07737 [cs].
URL: &lt;a href="http://arxiv.org/abs/1703.07737"&gt;http://arxiv.org/abs/1703.07737&lt;/a&gt; (visited on 2024-05-28), &lt;a href="https://doi.org/10.48550/arXiv.1703.07737"&gt;doi:10.48550/arXiv.1703.07737&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-hermans_defense_2017-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='khosla_supervised_2020'&gt;Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce&amp;nbsp;Liu, and Dilip Krishnan.
Supervised contrastive learning.
In &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt;, volume&amp;nbsp;33, 18661–18673. Curran Associates, Inc., 2020.
URL: &lt;a href="https://proceedings.neurips.cc/paper_files/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html"&gt;https://proceedings.neurips.cc/paper_files/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html&lt;/a&gt; (visited on 2024-03-20). &lt;a class="cite-backref" href="#ref-khosla_supervised_2020-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-khosla_supervised_2020-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-khosla_supervised_2020-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-khosla_supervised_2020-3" title="Jump back to reference 3"&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='koishekenov_geometric_2023'&gt;Yeskendir Koishekenov, Sharvaree Vadgama, Riccardo Valperga, and Erik&amp;nbsp;J. Bekkers.
Geometric contrastive learning.
In &lt;em&gt;2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)&lt;/em&gt;, 206–215. Paris, France, October 2023. IEEE.
URL: &lt;a href="https://ieeexplore.ieee.org/document/10350974/"&gt;https://ieeexplore.ieee.org/document/10350974/&lt;/a&gt; (visited on 2024-04-26), &lt;a href="https://doi.org/10.1109/ICCVW60793.2023.00028"&gt;doi:10.1109/ICCVW60793.2023.00028&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-koishekenov_geometric_2023-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;a class="cite-backref" href="#ref-koishekenov_geometric_2023-1" title="Jump back to reference 1"&gt; &lt;sup&gt;1&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-koishekenov_geometric_2023-2" title="Jump back to reference 2"&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;/a&gt;&lt;a class="cite-backref" href="#ref-koishekenov_geometric_2023-3" title="Jump back to reference 3"&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;/a&gt;&lt;/p&gt;
&lt;p id='le-khac_contrastive_2020'&gt;Phuc&amp;nbsp;H. Le-Khac, Graham Healy, and Alan&amp;nbsp;F. Smeaton.
Contrastive representation learning: a framework and review.
&lt;em&gt;IEEE Access&lt;/em&gt;, 8:193907–193934, 2020.
URL: &lt;a href="https://ieeexplore.ieee.org/document/9226466/"&gt;https://ieeexplore.ieee.org/document/9226466/&lt;/a&gt; (visited on 2023-11-09), &lt;a href="https://doi.org/10.1109/ACCESS.2020.3031549"&gt;doi:10.1109/ACCESS.2020.3031549&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-le-khac_contrastive_2020-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='oh_song_deep_2016'&gt;Hyun Oh&amp;nbsp;Song, Yu&amp;nbsp;Xiang, Stefanie Jegelka, and Silvio Savarese.
Deep metric learning via lifted structured feature embedding.
In &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;, 4004–4012. 2016.
URL: &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Song_Deep_Metric_Learning_CVPR_2016_paper.html"&gt;https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Song_Deep_Metric_Learning_CVPR_2016_paper.html&lt;/a&gt; (visited on 2024-05-28). &lt;a class="cite-backref" href="#ref-oh_song_deep_2016-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='oord_representation_2019'&gt;Aaron van&amp;nbsp;den Oord, Yazhe Li, and Oriol Vinyals.
Representation learning with contrastive predictive coding.
January 2019.
arXiv:1807.03748 [cs, stat].
URL: &lt;a href="http://arxiv.org/abs/1807.03748"&gt;http://arxiv.org/abs/1807.03748&lt;/a&gt; (visited on 2024-05-28), &lt;a href="https://doi.org/10.48550/arXiv.1807.03748"&gt;doi:10.48550/arXiv.1807.03748&lt;/a&gt;. &lt;a class="cite-backref" href="#ref-oord_representation_2019-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='schroff_facenet_2015'&gt;Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: a unified embedding for face recognition and clustering.
In &lt;em&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/em&gt;, 815–823. 2015.
URL: &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html"&gt;https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html&lt;/a&gt; (visited on 2024-05-28). &lt;a class="cite-backref" href="#ref-schroff_facenet_2015-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p id='wang_understanding_nodate'&gt;Tongzhou Wang and Phillip Isola.
Understanding contrastive representation learning through alignment and uniformity on the hypersphere.
2020. &lt;a class="cite-backref" href="#ref-wang_understanding_nodate-1" title="Jump back to reference 1"&gt;↩&lt;/a&gt;&lt;/p&gt;
</content><category term="blog"></category></entry><entry><title>Lecturer for Discrete Mathematics at Tufts University</title><link href="https://patrickfeeney.github.io/lecturer-for-discrete-mathematics-at-tufts-university.html" rel="alternate"></link><published>2024-05-22T10:00:00-05:00</published><updated>2024-05-22T10:00:00-05:00</updated><author><name>Patrick Feeney</name></author><id>tag:patrickfeeney.github.io,2024-05-22:/lecturer-for-discrete-mathematics-at-tufts-university.html</id><content type="html">&lt;p&gt;As a lecturer for the summer of 2024, I gave active-learning focused lectures, developed exams,  and held office hours for CS 61.
The course covered proof writing, propositional logic, combinatorics, relations, graph theory, and abstract algebra.&lt;/p&gt;</content><category term="courses"></category><category term="news"></category></entry><entry><title>A Neurosymbolic Cognitive Architecture Framework for Handling Novelties in Open Worlds</title><link href="https://patrickfeeney.github.io/a-neurosymbolic-cognitive-architecture-framework-for-handling-novelties-in-open-worlds.html" rel="alternate"></link><published>2024-03-15T10:00:00-05:00</published><updated>2024-03-15T10:00:00-05:00</updated><author><name>Patrick Feeney</name></author><id>tag:patrickfeeney.github.io,2024-03-15:/a-neurosymbolic-cognitive-architecture-framework-for-handling-novelties-in-open-worlds.html</id><summary type="html">&lt;p&gt;Published in &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S000437022400047X"&gt;Artificial Intelligence&lt;/a&gt;. &lt;a href="https://patrickfeeney.github.io/papers/Revised_v2_0__AIJ_23__A_Neurosymbolic_Cognitive_Architecture_for_Handling_Novelties_in_Open_Worlds.pdf"&gt;PDF&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;"Open world" environments are those in which novel objects, agents, events, and more can appear and contradict previous understandings of the environment. This contradicts the "closed world" assumption used in most AI research, where the environment is assumed to be fully understood and unchanging. The …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Published in &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S000437022400047X"&gt;Artificial Intelligence&lt;/a&gt;. &lt;a href="https://patrickfeeney.github.io/papers/Revised_v2_0__AIJ_23__A_Neurosymbolic_Cognitive_Architecture_for_Handling_Novelties_in_Open_Worlds.pdf"&gt;PDF&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;"Open world" environments are those in which novel objects, agents, events, and more can appear and contradict previous understandings of the environment. This contradicts the "closed world" assumption used in most AI research, where the environment is assumed to be fully understood and unchanging. The types of environments AI agents can be deployed in are limited by the inability to handle the novelties that occur in open world environments. This paper presents a novel cognitive architecture framework to handle open-world novelties. This framework combines symbolic planning, counterfactual reasoning, reinforcement learning, and deep computer vision to detect and accommodate novelties. We introduce general algorithms for exploring open worlds using inference and machine learning methodologies to facilitate novelty accommodation. The ability to detect and accommodate novelties allows agents built on this framework to successfully complete tasks despite a variety of novel changes to the world. Both the framework components and the entire system are evaluated in Minecraft-like simulated environments. Our results indicate that agents are able to efficiently complete tasks while accommodating "concealed novelties: not shared with the architecture development team.&lt;/p&gt;</content><category term="papers"></category><category term="news"></category></entry><entry><title>SINCERE: Supervised Information Noise-Contrastive Estimation REvisited</title><link href="https://patrickfeeney.github.io/sincere-supervised-information-noise-contrastive-estimation-revisited.html" rel="alternate"></link><published>2024-03-11T10:00:00-05:00</published><updated>2024-03-11T10:00:00-05:00</updated><author><name>Patrick Feeney</name></author><id>tag:patrickfeeney.github.io,2024-03-11:/sincere-supervised-information-noise-contrastive-estimation-revisited.html</id><summary type="html">&lt;p&gt;Under review for ICML 2024.
&lt;a href="https://arxiv.org/abs/2309.14277"&gt;arXiv version&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The information noise-contrastive estimation (InfoNCE) loss function provides the basis of many self-supervised deep learning methods due to its strong empirical results and theoretic motivation.
Previous work suggests a supervised contrastive (SupCon) loss to extend InfoNCE to learn from available class labels.
However …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Under review for ICML 2024.
&lt;a href="https://arxiv.org/abs/2309.14277"&gt;arXiv version&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The information noise-contrastive estimation (InfoNCE) loss function provides the basis of many self-supervised deep learning methods due to its strong empirical results and theoretic motivation.
Previous work suggests a supervised contrastive (SupCon) loss to extend InfoNCE to learn from available class labels.
However, in this work we find that the prior SupCon loss formulation has questionable justification because it can encourage some images from the same class to repel one another in the learned embedding space.
We propose the Supervised InfoNCE REvisited (SINCERE) loss as a theoretically-justified supervised extension of InfoNCE that never causes images from the same class to repel one another.
Experiments show that SINCERE leads to better separation of embeddings from different classes while delivering competitive classification accuracy for supervised and transfer learning.
We further show an information-theoretic bound that relates SINCERE loss to the symmeterized KL divergence between data-generating distributions for a target class and all other classes. &lt;/p&gt;</content><category term="papers"></category><category term="news"></category></entry><entry><title>Teaching Assistant for Statistical Pattern Recognition at Tufts University</title><link href="https://patrickfeeney.github.io/teaching-assistant-for-statistical-pattern-recognition-at-tufts-university.html" rel="alternate"></link><published>2024-01-11T10:00:00-05:00</published><updated>2024-01-11T10:00:00-05:00</updated><author><name>Patrick Feeney</name></author><id>tag:patrickfeeney.github.io,2024-01-11:/teaching-assistant-for-statistical-pattern-recognition-at-tufts-university.html</id><content type="html">&lt;p&gt;As a teaching assistant for the spring of 2024, I developed various assignments, graded, and held office hours for CS 136.
The course covered Bayesian regression, sampling techniques, directed graphical models, mixture models, and clustering.&lt;/p&gt;</content><category term="courses"></category><category term="news"></category></entry><entry><title>Teaching Assistant for Intro to Machine Learning at Tufts University</title><link href="https://patrickfeeney.github.io/teaching-assistant-for-intro-to-machine-learning-at-tufts-university.html" rel="alternate"></link><published>2023-09-01T10:00:00-05:00</published><updated>2023-09-01T10:00:00-05:00</updated><author><name>Patrick Feeney</name></author><id>tag:patrickfeeney.github.io,2023-09-01:/teaching-assistant-for-intro-to-machine-learning-at-tufts-university.html</id><content type="html">&lt;p&gt;As a teaching assistant for the fall of 2023, I developed various assignments, graded, and held office hours for CS 135.
The course covered regression, classification, neural networks, trees, ensembles, kernel methods, recommendation systems, and dimensionality reduction.&lt;/p&gt;</content><category term="courses"></category><category term="news"></category></entry><entry><title>NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds</title><link href="https://patrickfeeney.github.io/novelcraft-a-dataset-for-novelty-detection-and-discovery-in-open-worlds.html" rel="alternate"></link><published>2022-06-23T10:00:00-05:00</published><updated>2022-06-23T10:00:00-05:00</updated><author><name>Patrick Feeney</name></author><id>tag:patrickfeeney.github.io,2022-06-23:/novelcraft-a-dataset-for-novelty-detection-and-discovery-in-open-worlds.html</id><summary type="html">&lt;p&gt;Published in &lt;a href="https://openreview.net/forum?id=4eL6z9ziw7"&gt;TMLR&lt;/a&gt;.
&lt;a href="https://arxiv.org/abs/2206.11736"&gt;arXiv version&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Published in &lt;a href="https://openreview.net/forum?id=4eL6z9ziw7"&gt;TMLR&lt;/a&gt;.
&lt;a href="https://arxiv.org/abs/2206.11736"&gt;arXiv version&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus on one distinct, well-centered object. New benchmarks are needed to represent the challenges of navigating the complex scenes of an open world. Our new NovelCraft dataset contains multimodal episodic data of the images and symbolic world-states seen by an agent completing a pogo stick assembly task within a modified Minecraft environment. In some episodes, we insert novel objects of varying size within the complex 3D scene that may impact gameplay. Our visual novelty detection benchmark finds that methods that rank best on popular area-under-the-curve metrics may be outperformed by simpler alternatives when controlling false positives matters most. Further multimodal novelty detection experiments suggest that methods that fuse both visual and symbolic information can improve time until detection as well as overall discrimination. Finally, our evaluation of recent generalized category discovery methods suggests that adapting to new imbalanced categories in complex scenes remains an exciting open problem.&lt;/p&gt;</content><category term="papers"></category><category term="news"></category></entry><entry><title>Evaluating the Use of Reconstruction Error for Novelty Localization</title><link href="https://patrickfeeney.github.io/evaluating-the-use-of-reconstruction-error-for-novelty-localization.html" rel="alternate"></link><published>2021-07-28T10:00:00-05:00</published><updated>2021-07-28T10:00:00-05:00</updated><author><name>Patrick Feeney</name></author><id>tag:patrickfeeney.github.io,2021-07-28:/evaluating-the-use-of-reconstruction-error-for-novelty-localization.html</id><summary type="html">&lt;p&gt;Published in &lt;a href="https://icml.cc/virtual/2021/workshop/8374"&gt;ICML 2021 Workshop on Uncertainty and Robustness in Deep Learning&lt;/a&gt;.
&lt;a href="https://arxiv.org/abs/2107.13379"&gt;arXiv version&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The pixelwise reconstruction error of deep autoencoders is often utilized for image novelty detection and localization under the assumption that pixels with high error indicate which parts of the input image are unfamiliar and therefore likely …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Published in &lt;a href="https://icml.cc/virtual/2021/workshop/8374"&gt;ICML 2021 Workshop on Uncertainty and Robustness in Deep Learning&lt;/a&gt;.
&lt;a href="https://arxiv.org/abs/2107.13379"&gt;arXiv version&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The pixelwise reconstruction error of deep autoencoders is often utilized for image novelty detection and localization under the assumption that pixels with high error indicate which parts of the input image are unfamiliar and therefore likely to be novel. This assumed correlation between pixels with high reconstruction error and novel regions of input images has not been verified and may limit the accuracy of these methods. In this paper we utilize saliency maps to evaluate whether this correlation exists. Saliency maps reveal directly how much a change in each input pixel would affect reconstruction loss, while each pixel's reconstruction error may be attributed to many input pixels when layers are fully connected. We compare saliency maps to reconstruction error maps via qualitative visualizations as well as quantitative correspondence between the top K elements of the maps for both novel and normal images. Our results indicate that reconstruction error maps do not closely correlate with the importance of pixels in the input images, making them insufficient for novelty localization.&lt;/p&gt;</content><category term="papers"></category><category term="news"></category></entry><entry><title>4-d Scene Alignment in Surveillance Video</title><link href="https://patrickfeeney.github.io/4-d-scene-alignment-in-surveillance-video.html" rel="alternate"></link><published>2019-10-15T10:00:00-05:00</published><updated>2019-10-15T10:00:00-05:00</updated><author><name>Patrick Feeney</name></author><id>tag:patrickfeeney.github.io,2019-10-15:/4-d-scene-alignment-in-surveillance-video.html</id><summary type="html">&lt;p&gt;Published in &lt;a href="https://ieeexplore.ieee.org/abstract/document/9174582"&gt;2019 IEEE Applied Imagery Pattern Recognition Workshop&lt;/a&gt;.
&lt;a href="https://arxiv.org/abs/1906.01675"&gt;arXiv version&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Designing robust activity detectors for fixed camera surveillance video requires knowledge of the 3-D scene. This paper presents an automatic camera calibration process that provides a mechanism to reason about the spatial proximity between objects at different times. It …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Published in &lt;a href="https://ieeexplore.ieee.org/abstract/document/9174582"&gt;2019 IEEE Applied Imagery Pattern Recognition Workshop&lt;/a&gt;.
&lt;a href="https://arxiv.org/abs/1906.01675"&gt;arXiv version&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Designing robust activity detectors for fixed camera surveillance video requires knowledge of the 3-D scene. This paper presents an automatic camera calibration process that provides a mechanism to reason about the spatial proximity between objects at different times. It combines a CNN-based camera pose estimator with a vertical scale provided by pedestrian observations to establish the 4-D scene geometry. Unlike some previous methods, the people do not need to be tracked nor do the head and feet need to be explicitly detected. It is robust to individual height variations and camera parameter estimation errors.&lt;/p&gt;</content><category term="papers"></category><category term="news"></category></entry></feed>